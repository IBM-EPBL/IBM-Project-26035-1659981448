{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "leb16xLl6UZR",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-06 17:48:45.866285: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-06 17:48:46.554051: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-10-06 17:48:46.826112: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-06 17:48:46.826149: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-06 17:48:46.993023: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-06 17:48:50.003532: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-06 17:48:50.004483: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-06 17:48:50.004517: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lM_Y4H_a6fGH"
   },
   "outputs": [],
   "source": [
    "train_datagen=ImageDataGenerator(rescale=1./255,shear_range=0.2,\n",
    "                                 zoom_range=0.2,horizontal_flip=True)\n",
    "test_datagen=ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "emcSFZNf6oLb",
    "outputId": "fdd803fd-915d-4842-bafe-aa9b78dea4bb"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Flowers-Dataset/flowers/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/local/ZOHOCORP/yuvaraj-pt5649/Downloads/Assignment 3_training model.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/local/ZOHOCORP/yuvaraj-pt5649/Downloads/Assignment%203_training%20model.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m xtrain\u001b[39m=\u001b[39mtrain_datagen\u001b[39m.\u001b[39;49mflow_from_directory(\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mFlowers-Dataset/flowers/train\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/local/ZOHOCORP/yuvaraj-pt5649/Downloads/Assignment%203_training%20model.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                                          target_size\u001b[39m=\u001b[39;49m(\u001b[39m64\u001b[39;49m, \u001b[39m64\u001b[39;49m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/local/ZOHOCORP/yuvaraj-pt5649/Downloads/Assignment%203_training%20model.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                                           batch_size\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,                                           \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/local/ZOHOCORP/yuvaraj-pt5649/Downloads/Assignment%203_training%20model.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                                           class_mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcategorical\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/local/ZOHOCORP/yuvaraj-pt5649/Downloads/Assignment%203_training%20model.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                                          )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/preprocessing/image.py:1650\u001b[0m, in \u001b[0;36mImageDataGenerator.flow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflow_from_directory\u001b[39m(\n\u001b[1;32m   1565\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1566\u001b[0m     directory,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1580\u001b[0m     keep_aspect_ratio\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1581\u001b[0m ):\n\u001b[1;32m   1582\u001b[0m     \u001b[39m\"\"\"Takes the path to a directory & generates batches of augmented data.\u001b[39;00m\n\u001b[1;32m   1583\u001b[0m \n\u001b[1;32m   1584\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[39m            and `y` is a numpy array of corresponding labels.\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1650\u001b[0m     \u001b[39mreturn\u001b[39;00m DirectoryIterator(\n\u001b[1;32m   1651\u001b[0m         directory,\n\u001b[1;32m   1652\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1653\u001b[0m         target_size\u001b[39m=\u001b[39;49mtarget_size,\n\u001b[1;32m   1654\u001b[0m         color_mode\u001b[39m=\u001b[39;49mcolor_mode,\n\u001b[1;32m   1655\u001b[0m         keep_aspect_ratio\u001b[39m=\u001b[39;49mkeep_aspect_ratio,\n\u001b[1;32m   1656\u001b[0m         classes\u001b[39m=\u001b[39;49mclasses,\n\u001b[1;32m   1657\u001b[0m         class_mode\u001b[39m=\u001b[39;49mclass_mode,\n\u001b[1;32m   1658\u001b[0m         data_format\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_format,\n\u001b[1;32m   1659\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   1660\u001b[0m         shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m   1661\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m   1662\u001b[0m         save_to_dir\u001b[39m=\u001b[39;49msave_to_dir,\n\u001b[1;32m   1663\u001b[0m         save_prefix\u001b[39m=\u001b[39;49msave_prefix,\n\u001b[1;32m   1664\u001b[0m         save_format\u001b[39m=\u001b[39;49msave_format,\n\u001b[1;32m   1665\u001b[0m         follow_links\u001b[39m=\u001b[39;49mfollow_links,\n\u001b[1;32m   1666\u001b[0m         subset\u001b[39m=\u001b[39;49msubset,\n\u001b[1;32m   1667\u001b[0m         interpolation\u001b[39m=\u001b[39;49minterpolation,\n\u001b[1;32m   1668\u001b[0m         dtype\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype,\n\u001b[1;32m   1669\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/preprocessing/image.py:563\u001b[0m, in \u001b[0;36mDirectoryIterator.__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m classes:\n\u001b[1;32m    562\u001b[0m     classes \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 563\u001b[0m     \u001b[39mfor\u001b[39;00m subdir \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(os\u001b[39m.\u001b[39;49mlistdir(directory)):\n\u001b[1;32m    564\u001b[0m         \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(directory, subdir)):\n\u001b[1;32m    565\u001b[0m             classes\u001b[39m.\u001b[39mappend(subdir)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Flowers-Dataset/flowers/train'"
     ]
    }
   ],
   "source": [
    "xtrain=train_datagen.flow_from_directory(r'Flowers-Dataset/flowers/train',\n",
    "                                         target_size=(64, 64),\n",
    "                                          batch_size=100,                                           \n",
    "                                          class_mode='categorical'\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F-maVOCo6yCE",
    "outputId": "bc86a1ed-0be8-44c3-a14e-fb0f18df0f0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "xtest=test_datagen.flow_from_directory(r'Flowers-Dataset/flowers/test',\n",
    "                                          target_size=(64, 64),\n",
    "                                          batch_size=100,                                           \n",
    "                                          class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNTQ9uwS67LB"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Convolution2D,MaxPool2D,Flatten,Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RF34Fjk37Bih"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(32,(3,3),activation='relu',input_shape=(64,64,3))) \n",
    "model.add(MaxPool2D(pool_size=(2,2))) \n",
    "model.add(Flatten()) \n",
    "model.add(Dense(300,activation='relu')) \n",
    "model.add(Dense(150,activation='relu')) \n",
    "model.add(Dense(5,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_HuuemWP7Ewg"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhbLviu3IL99"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5850/371111531.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(xtrain,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "44/44 [==============================] - 12s 273ms/step - loss: 1.4680 - accuracy: 0.4058 - val_loss: 1.0450 - val_accuracy: 0.3704\n",
      "Epoch 2/30\n",
      "44/44 [==============================] - 14s 317ms/step - loss: 1.1067 - accuracy: 0.5527 - val_loss: 1.0041 - val_accuracy: 0.6296\n",
      "Epoch 3/30\n",
      "44/44 [==============================] - 14s 322ms/step - loss: 1.0130 - accuracy: 0.5997 - val_loss: 0.7982 - val_accuracy: 0.6296\n",
      "Epoch 4/30\n",
      "44/44 [==============================] - 17s 382ms/step - loss: 0.9442 - accuracy: 0.6328 - val_loss: 0.7864 - val_accuracy: 0.7037\n",
      "Epoch 5/30\n",
      "44/44 [==============================] - 15s 332ms/step - loss: 0.8792 - accuracy: 0.6623 - val_loss: 0.9963 - val_accuracy: 0.7778\n",
      "Epoch 6/30\n",
      "44/44 [==============================] - 18s 404ms/step - loss: 0.8547 - accuracy: 0.6690 - val_loss: 0.8110 - val_accuracy: 0.7037\n",
      "Epoch 7/30\n",
      "44/44 [==============================] - 16s 350ms/step - loss: 0.7990 - accuracy: 0.7007 - val_loss: 0.6435 - val_accuracy: 0.8148\n",
      "Epoch 8/30\n",
      "44/44 [==============================] - 15s 343ms/step - loss: 0.7546 - accuracy: 0.7142 - val_loss: 0.7294 - val_accuracy: 0.7407\n",
      "Epoch 9/30\n",
      "44/44 [==============================] - 18s 396ms/step - loss: 0.7123 - accuracy: 0.7313 - val_loss: 0.6709 - val_accuracy: 0.7778\n",
      "Epoch 10/30\n",
      "44/44 [==============================] - 18s 407ms/step - loss: 0.6852 - accuracy: 0.7438 - val_loss: 0.6060 - val_accuracy: 0.7778\n",
      "Epoch 11/30\n",
      "44/44 [==============================] - 15s 334ms/step - loss: 0.6674 - accuracy: 0.7417 - val_loss: 0.6927 - val_accuracy: 0.6667\n",
      "Epoch 12/30\n",
      "44/44 [==============================] - 17s 378ms/step - loss: 0.6334 - accuracy: 0.7670 - val_loss: 0.6090 - val_accuracy: 0.7778\n",
      "Epoch 13/30\n",
      "44/44 [==============================] - 15s 338ms/step - loss: 0.6083 - accuracy: 0.7714 - val_loss: 0.4950 - val_accuracy: 0.8519\n",
      "Epoch 14/30\n",
      "44/44 [==============================] - 17s 377ms/step - loss: 0.5675 - accuracy: 0.7855 - val_loss: 0.4593 - val_accuracy: 0.8519\n",
      "Epoch 15/30\n",
      "44/44 [==============================] - 15s 347ms/step - loss: 0.5539 - accuracy: 0.7890 - val_loss: 0.5288 - val_accuracy: 0.8519\n",
      "Epoch 16/30\n",
      "44/44 [==============================] - 15s 334ms/step - loss: 0.5261 - accuracy: 0.8043 - val_loss: 0.4308 - val_accuracy: 0.8889\n",
      "Epoch 17/30\n",
      "44/44 [==============================] - 16s 375ms/step - loss: 0.4869 - accuracy: 0.8205 - val_loss: 0.6826 - val_accuracy: 0.8148\n",
      "Epoch 18/30\n",
      "44/44 [==============================] - 15s 333ms/step - loss: 0.4781 - accuracy: 0.8274 - val_loss: 0.4648 - val_accuracy: 0.8148\n",
      "Epoch 19/30\n",
      "44/44 [==============================] - 15s 344ms/step - loss: 0.4325 - accuracy: 0.8453 - val_loss: 0.2542 - val_accuracy: 0.8889\n",
      "Epoch 20/30\n",
      "44/44 [==============================] - 14s 306ms/step - loss: 0.4402 - accuracy: 0.8404 - val_loss: 0.2697 - val_accuracy: 0.9259\n",
      "Epoch 21/30\n",
      "44/44 [==============================] - 15s 348ms/step - loss: 0.4141 - accuracy: 0.8531 - val_loss: 0.3391 - val_accuracy: 0.7778\n",
      "Epoch 22/30\n",
      "44/44 [==============================] - 16s 375ms/step - loss: 0.4195 - accuracy: 0.8527 - val_loss: 0.3712 - val_accuracy: 0.8148\n",
      "Epoch 23/30\n",
      "44/44 [==============================] - 16s 357ms/step - loss: 0.3524 - accuracy: 0.8816 - val_loss: 0.3969 - val_accuracy: 0.8519\n",
      "Epoch 24/30\n",
      "44/44 [==============================] - 17s 400ms/step - loss: 0.3484 - accuracy: 0.8747 - val_loss: 0.5674 - val_accuracy: 0.8148\n",
      "Epoch 25/30\n",
      "44/44 [==============================] - 14s 321ms/step - loss: 0.3002 - accuracy: 0.8976 - val_loss: 0.4071 - val_accuracy: 0.8519\n",
      "Epoch 26/30\n",
      "44/44 [==============================] - 13s 299ms/step - loss: 0.2927 - accuracy: 0.8967 - val_loss: 0.2987 - val_accuracy: 0.8519\n",
      "Epoch 27/30\n",
      "44/44 [==============================] - 14s 320ms/step - loss: 0.3122 - accuracy: 0.8837 - val_loss: 0.1558 - val_accuracy: 0.9630\n",
      "Epoch 28/30\n",
      "44/44 [==============================] - 15s 340ms/step - loss: 0.2895 - accuracy: 0.8983 - val_loss: 0.2189 - val_accuracy: 0.8889\n",
      "Epoch 29/30\n",
      "44/44 [==============================] - 15s 351ms/step - loss: 0.2702 - accuracy: 0.9092 - val_loss: 0.3741 - val_accuracy: 0.8148\n",
      "Epoch 30/30\n",
      "44/44 [==============================] - 13s 302ms/step - loss: 0.2515 - accuracy: 0.9168 - val_loss: 0.1828 - val_accuracy: 0.8889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f80dc1ab520>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(xtrain,\n",
    "                    steps_per_epoch=len(xtrain),\n",
    "                    epochs=30,\n",
    "                    validation_data=xtest,\n",
    "                    validation_steps=len(xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('flower.h5')\n",
    "model_json=model.to_json()\n",
    "with open(\"model-bw.json\",\"w\") as json_file:json_file.write(model_json)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "97cc609b13305c559618ec78a438abc56230b9381f827f22d070313b9a1f3777"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
